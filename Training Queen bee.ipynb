{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Training Reine","provenance":[{"file_id":"1DNZluuX_Rp702PhUO8qor6UYs1mUBYay","timestamp":1604817620659},{"file_id":"1gDZ2xcTOgR39tGGs-EZ6i3RTs16wmzZQ","timestamp":1600577710129},{"file_id":"https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb","timestamp":1591755516488}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"GD9gUQpaBxNa"},"source":["# Entrainer YOLOv5 sur des abeilles\n","Dans ce projet nous allons utiliser yolo version 5 de \"ultralytics repository\" accessible depuis https://github.com/ultralytics/yolov5 appliquer à notre dataset qui constitue les deux classes _\"bee\"_ et _\"queen\"_ afin d'identifier et de localiser la reine d'abeille au milieu de sa colonie.\n","Pour entrainer notre detecteur voici les étapes que l'on va suivre dans ce tutoriel:\n","\n","* Installer les dependences de YOLOv5 \n","* Télécharger notre YOLOv5 objet detection personaliser depuis notre repertoire github\n","* Ecrire notre YOLOv5 configuration d'apprentissage\n","* Executer l'entrainement YOLOv5\n","* Evaluer la performance de notre detection par YOLOv5 \n","* Exporter le YOLOv5 poids(weights) sur google drive\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7mGmQbAO5pQb"},"source":["#Téléchargement des données et installations des dépendences\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"79zuSWI5WORY","executionInfo":{"status":"ok","timestamp":1623118652479,"user_tz":-240,"elapsed":393,"user":{"displayName":"RASAMBATRA Emmanuel Fério","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKLs7YeNKHVJRfcfdxGqFrgOekh0u0b1ixnYW2JQ=s64","userId":"12601890248843483443"}},"outputId":"3784dd05-1030-46a2-ec27-0af1dac9f913"},"source":["!pwd"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/QueenBee_Training/yolov5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wbvMlHd_QwMG","executionInfo":{"status":"ok","timestamp":1623117004880,"user_tz":-240,"elapsed":11979,"user":{"displayName":"RASAMBATRA Emmanuel Fério","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKLs7YeNKHVJRfcfdxGqFrgOekh0u0b1ixnYW2JQ=s64","userId":"12601890248843483443"}},"outputId":"d025bf1d-2088-4ca3-9a5b-020de5728fc3"},"source":["# Export code snippet and paste here\n","!git clone \"https://github.com/ferio06/QueenBee_Training.git\"\n","%cd QueenBee_Training/\n","!unzip DataBaseQueen.zip; rm DataBaseQueen.zip\n","!unzip yolov5.zip; rm yolov5.zip\n","!pip install -qr yolov5/requirements.txt  # install dependencies (ignore errors)\n","%cd yolov5\n","import torch\n","from IPython.display import Image, clear_output  # pour afficher les images\n","from utils.google_utils import gdrive_download  # pour télécharger les models/datasets\n","\n","clear_output()\n","\n","print('Ici on utilise torch %s \\n \\nMateriel utiliser : \\n   > %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Ici on utilise torch 1.8.1+cu101 \n"," \n","Materiel utiliser : \n","   > _CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15109MB, multi_processor_count=40)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZZ3DmmGQztJj","executionInfo":{"elapsed":15249,"status":"ok","timestamp":1607252993605,"user":{"displayName":"RASAMBATRA Emmanuel Fério","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKLs7YeNKHVJRfcfdxGqFrgOekh0u0b1ixnYW2JQ=s64","userId":"12601890248843483443"},"user_tz":-240},"outputId":"d3fabae7-a3ea-4b71-de3e-5187337b0e2f"},"source":["# Voici le fichier YAML que Roboflow a écrit contenant les informations sur les classes de notre dataset\n","%cat ../data.yaml"],"execution_count":null,"outputs":[{"output_type":"stream","text":["train: ../train/images\n","val: ../valid/images\n","\n","nc: 2\n","names: ['bee', 'queen']"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UwJx-2NHsYxT"},"source":["# Definir la modèle de configuration et architecture\n","\n","Ici on va écrire un script yaml script definissant les parametres pour notre modèle tel que le nombre de classe, anchors, et chaque couche."]},{"cell_type":"code","metadata":{"id":"dOPn9wjOAwwK"},"source":["# définir le nombre de classe basé sur le fichier YAML\n","import yaml\n","with open(\"../data.yaml\", 'r') as stream:\n","    num_classes = str(yaml.safe_load(stream)['nc'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Rvt5wilnDyX","executionInfo":{"elapsed":15241,"status":"ok","timestamp":1607252993628,"user":{"displayName":"RASAMBATRA Emmanuel Fério","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiKLs7YeNKHVJRfcfdxGqFrgOekh0u0b1ixnYW2JQ=s64","userId":"12601890248843483443"},"user_tz":-240},"outputId":"0d6fd077-6cde-4fbe-801b-29eb93d71876"},"source":["#Pour ce projet nous allons utilisé le modèle de configuration suivant  \n","%cat /content/QueenBee_Training/yolov5/models/yolov5x.yaml"],"execution_count":null,"outputs":[{"output_type":"stream","text":["# parameters\n","nc: 80  # number of classes\n","depth_multiple: 1.33  # model depth multiple\n","width_multiple: 1.25  # layer channel multiple\n","\n","# anchors\n","anchors:\n","  - [10,13, 16,30, 33,23]  # P3/8\n","  - [30,61, 62,45, 59,119]  # P4/16\n","  - [116,90, 156,198, 373,326]  # P5/32\n","\n","# YOLOv5 backbone\n","backbone:\n","  # [from, number, module, args]\n","  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n","   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n","   [-1, 3, BottleneckCSP, [128]],\n","   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n","   [-1, 9, BottleneckCSP, [256]],\n","   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n","   [-1, 9, BottleneckCSP, [512]],\n","   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n","   [-1, 1, SPP, [1024, [5, 9, 13]]],\n","   [-1, 3, BottleneckCSP, [1024, False]],  # 9\n","  ]\n","\n","# YOLOv5 head\n","head:\n","  [[-1, 1, Conv, [512, 1, 1]],\n","   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n","   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n","   [-1, 3, BottleneckCSP, [512, False]],  # 13\n","\n","   [-1, 1, Conv, [256, 1, 1]],\n","   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n","   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n","   [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)\n","\n","   [-1, 1, Conv, [256, 3, 2]],\n","   [[-1, 14], 1, Concat, [1]],  # cat head P4\n","   [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium)\n","\n","   [-1, 1, Conv, [512, 3, 2]],\n","   [[-1, 10], 1, Concat, [1]],  # cat head P5\n","   [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5/32-large)\n","\n","   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n","  ]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t14hhyqdmw6O"},"source":["#Personnaliser iPython writefile pour nous permetre d'écrire nos variables\n","from IPython.core.magic import register_line_cell_magic\n","\n","@register_line_cell_magic\n","def writetemplate(line, cell):\n","    with open(line, 'w') as f:\n","        f.write(cell.format(**globals()))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uDxebz13RdRA"},"source":["%%writetemplate /content/QueenBee_Training/yolov5/models/custom_yolov5s.yaml\n","\n","# parametres\n","nc: 2  # nombre de classe\n","depth_multiple: 1.33  # model depth multiple\n","width_multiple: 1.25  # layer channel multiple\n","\n","# anchors\n","anchors:\n","  - [10,13, 16,30, 33,23]  # P3/8\n","  - [30,61, 62,45, 59,119]  # P4/16\n","  - [116,90, 156,198, 373,326]  # P5/32\n","\n","# YOLOv5 backbone\n","backbone:\n","  # [from, number, module, args]\n","  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n","   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n","   [-1, 3, BottleneckCSP, [128]],\n","   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n","   [-1, 9, BottleneckCSP, [256]],\n","   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n","   [-1, 9, BottleneckCSP, [512]],\n","   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n","   [-1, 1, SPP, [1024, [5, 9, 13]]],\n","   [-1, 3, BottleneckCSP, [1024, False]],  # 9\n","  ]\n","\n","# YOLOv5 head\n","head:\n","  [[-1, 1, Conv, [512, 1, 1]],\n","   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n","   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n","   [-1, 3, BottleneckCSP, [512, False]],  # 13\n","\n","   [-1, 1, Conv, [256, 1, 1]],\n","   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n","   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n","   [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)\n","\n","   [-1, 1, Conv, [256, 3, 2]],\n","   [[-1, 14], 1, Concat, [1]],  # cat head P4\n","   [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium)\n","\n","   [-1, 1, Conv, [512, 3, 2]],\n","   [[-1, 10], 1, Concat, [1]],  # cat head P5\n","   [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5/32-large)\n","\n","   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n","  ]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VUOiNLtMP5aG"},"source":["# Entrainer notre détecteur YOLOv5 \n","\n","C'est ici qu'on fait passer les nombres d'arguments :\n","- **img:** définit la taille d'image d'entrer\n","- **batch:** détermine la taille du batch \n","- **epochs:** definit le numbre d' epochs d'entrainement. \n","- **data:** specifier le chemin pour notre fichier yaml\n","- **cfg:** specifier notre model de configuration\n","- **weights:** specifier le chemin d'accès que l'on a personnaliser au weights. \n","- **name:** nom donné au resultat\n","- **nosave:** sauvegarder seulement le checkpoint finale.\n","- **cache:** la cache des images pour une vitesse d'entrainement plus rapide."]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"1NcFxRcFdJ_O"},"source":["%%time\n","%cd /content/QueenBee_Training/yolov5/\n","!python train.py --img 416 --batch 5 --epochs 200 --data '../data.yaml' --cfg ./models/yolov5x.yaml --weights '' --name yolov5x_50_200  --cache"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kJVs_4zEeVbF"},"source":["# Evaluation de la performance de notre detecteur YOLOv5"]},{"cell_type":"markdown","metadata":{"id":"7KN5ghjE6ZWh"},"source":[""]},{"cell_type":"code","metadata":{"id":"bOy5KI2ncnWd"},"source":["%load_ext tensorboard\n","%tensorboard --logdir runs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9nmZZnWOgJ2S"},"source":["%cd /content/QueenBee_Training/yolov5/\n","!python detect.py --weights runs/exp0_yolov5x_15_200/weights/last.pt --img 416 --conf 0.4 --source ../test/images\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"odKEqYtTgbRc"},"source":["import glob\n","from IPython.display import Image, display\n","\n","for imageName in glob.glob('/content/QueenBee_Training/yolov5/inference/output/*.jpg'): \n","    display(Image(filename=imageName))\n","    print(\"\\n\")"],"execution_count":null,"outputs":[]}]}